{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7808dd89-2371-4820-8d7f-299a28abadda",
   "metadata": {},
   "source": [
    "# Part 1: Data Download and Preparation \n",
    "\n",
    "## Step 1: Download Intraday Market Data\n",
    "\n",
    "Objective: Acquire historical intraday data for a selected equity or cryptocurrency.\n",
    "\n",
    "Requirements:\n",
    "\n",
    "Use yfinance for equities (e.g., AAPL).\n",
    "Use a crypto API (e.g., Binance) for cryptocurrencies.\n",
    "Save data as CSV with columns: Datetime, Open, High, Low, Close, Volume.\n",
    "Deliverable:\n",
    "\n",
    "A CSV file containing intraday market data for the chosen asset.\n",
    "Example (Equity):\n",
    "\n",
    "python\n",
    "\n",
    "import yfinance as yf\n",
    "\n",
    "data = yf.download(tickers='AAPL', period='7d', interval='1m')\n",
    "\n",
    "data.to_csv('market_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47fed4e0-74ab-42b9-afa7-1801dcb00096",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0503acdc-7c99-4b0a-bcbb-992bf26b6493",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e659d017-0b40-4a7b-a1c8-b6d55abbd6a3",
   "metadata": {},
   "source": [
    "For our version of the end-to-end tradign system, we will use data on the top 10 stocks by market capitalization for now. Note that this can eaily be chnaged, and our use of the Alpaca API will provide additional support for cryptocurrencies and additional assest classes. For now, our focus will be the top 10 stocks mentioned here: https://finance.yahoo.com/research-hub/screener/largest_market_cap/?guccounter=1&guce_referrer=aHR0cHM6Ly93d3cuZ29vZ2xlLmNvbS8&guce_referrer_sig=AQAAAIv-Bw2JL9C_WgD2UYHX8tuGb-x89S-SvE_P5yZAdbRsLggdSzI64TaWt-4xTu8IaLG-rW0_loydqM6sFidpeQdyyiFd4kx11oM3-sqEWjsXMpgfjQyFBkrnXmiaN0MlV6wLBDP2eL7Z5rbOUaBtX5t2iJBhqRsbrrKX_OL0t1FI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cfe872bb-ad48-4fe8-b29b-b2ebb33a63cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[1/9] NVDA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3m/9vs2kbw5395dntphy60zph4h0000gn/T/ipykernel_42289/3484499035.py:28: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
      "  data = yf.download(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Rows: 2,814\n",
      "    Range: 2025-11-07 14:30:00+00:00 to 2025-11-18 15:53:00+00:00\n",
      "    Saved: market_data/NVDA_1m.csv\n",
      "\n",
      "[2/9] AAPL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3m/9vs2kbw5395dntphy60zph4h0000gn/T/ipykernel_42289/3484499035.py:28: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
      "  data = yf.download(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Rows: 2,814\n",
      "    Range: 2025-11-07 14:30:00+00:00 to 2025-11-18 15:53:00+00:00\n",
      "    Saved: market_data/AAPL_1m.csv\n",
      "\n",
      "[3/9] MSFT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3m/9vs2kbw5395dntphy60zph4h0000gn/T/ipykernel_42289/3484499035.py:28: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
      "  data = yf.download(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Rows: 2,812\n",
      "    Range: 2025-11-07 14:30:00+00:00 to 2025-11-18 15:53:00+00:00\n",
      "    Saved: market_data/MSFT_1m.csv\n",
      "\n",
      "[4/9] GOOGL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3m/9vs2kbw5395dntphy60zph4h0000gn/T/ipykernel_42289/3484499035.py:28: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
      "  data = yf.download(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Rows: 2,812\n",
      "    Range: 2025-11-07 14:30:00+00:00 to 2025-11-18 15:53:00+00:00\n",
      "    Saved: market_data/GOOGL_1m.csv\n",
      "\n",
      "[5/9] AMZN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3m/9vs2kbw5395dntphy60zph4h0000gn/T/ipykernel_42289/3484499035.py:28: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
      "  data = yf.download(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Rows: 2,814\n",
      "    Range: 2025-11-07 14:30:00+00:00 to 2025-11-18 15:53:00+00:00\n",
      "    Saved: market_data/AMZN_1m.csv\n",
      "\n",
      "[6/9] AVGO\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3m/9vs2kbw5395dntphy60zph4h0000gn/T/ipykernel_42289/3484499035.py:28: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
      "  data = yf.download(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Rows: 2,814\n",
      "    Range: 2025-11-07 14:30:00+00:00 to 2025-11-18 15:53:00+00:00\n",
      "    Saved: market_data/AVGO_1m.csv\n",
      "\n",
      "[7/9] META\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3m/9vs2kbw5395dntphy60zph4h0000gn/T/ipykernel_42289/3484499035.py:28: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
      "  data = yf.download(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Rows: 2,814\n",
      "    Range: 2025-11-07 14:30:00+00:00 to 2025-11-18 15:53:00+00:00\n",
      "    Saved: market_data/META_1m.csv\n",
      "\n",
      "[8/9] TSM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3m/9vs2kbw5395dntphy60zph4h0000gn/T/ipykernel_42289/3484499035.py:28: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
      "  data = yf.download(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Rows: 2,814\n",
      "    Range: 2025-11-07 14:30:00+00:00 to 2025-11-18 15:53:00+00:00\n",
      "    Saved: market_data/TSM_1m.csv\n",
      "\n",
      "[9/9] BRK-B\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3m/9vs2kbw5395dntphy60zph4h0000gn/T/ipykernel_42289/3484499035.py:28: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
      "  data = yf.download(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Rows: 2,814\n",
      "    Range: 2025-11-07 14:30:00+00:00 to 2025-11-18 15:53:00+00:00\n",
      "    Saved: market_data/BRK-B_1m.csv\n",
      "\n",
      "Sample CSV format:\n",
      "                    Datetime        Open        High         Low       Close   Volume\n",
      "0  2025-11-07 14:30:00+00:00  184.860001  184.860001  183.654999  184.139999  8109544\n",
      "1  2025-11-07 14:31:00+00:00  184.160004  185.229996  184.160004  184.899994  1052345\n",
      "2  2025-11-07 14:32:00+00:00  184.889999  185.550003  184.839996  185.538803   750338\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "- Note that both GOOGL and GOOG were included in the top 10 market-cap list, \n",
    "so GOOGL was selected since it was higher up to avoid representing the same company\n",
    "\n",
    "- Birkshire Hathaway (A) was also included in the list, but the share price was too high,\n",
    "so BRK-B was chosen instead\n",
    "\n",
    "Data Download Configuration:\n",
    "- Period: 7 days (yfinance typically allows 7-30 days of 1-minute data)\n",
    "- Interval: 1 minute bars\n",
    "- Each stock is downloaded separately to ensure proper CSV format\n",
    "'''\n",
    "\n",
    "TICKERS = ['NVDA', 'AAPL', 'MSFT', 'GOOGL', 'AMZN', 'AVGO', 'META', 'TSM', 'BRK-B']\n",
    "PERIOD = '8d'\n",
    "INTERVAL = '1m'\n",
    "OUTPUT_DIR = 'market_data'\n",
    "\n",
    "# Create output directory\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Download each ticker separately\n",
    "for i, ticker in enumerate(TICKERS, 1):\n",
    "    print(f\"\\n[{i}/{len(TICKERS)}] {ticker}\")\n",
    "    \n",
    "    try:\n",
    "        # Download data for single ticker\n",
    "        data = yf.download(\n",
    "            tickers=ticker,\n",
    "            period=PERIOD,\n",
    "            interval=INTERVAL,\n",
    "            progress=False\n",
    "        )\n",
    "        # Flatten MultiIndex columns if they exist\n",
    "        if isinstance(data.columns, pd.MultiIndex):\n",
    "            data.columns = data.columns.droplevel(1)\n",
    "        \n",
    "        # Verify required columns exist\n",
    "        required_cols = ['Open', 'High', 'Low', 'Close', 'Volume']\n",
    "        if not all(col in data.columns for col in required_cols):\n",
    "            print(f\"    Missing required columns\")\n",
    "            continue\n",
    "        \n",
    "        # Select only required columns in correct order\n",
    "        data = data[required_cols]\n",
    "        \n",
    "        # Save to CSV with Datetime as column\n",
    "        output_file = f\"{OUTPUT_DIR}/{ticker}_1m.csv\"\n",
    "        data.to_csv(output_file)\n",
    "        \n",
    "        # Verification\n",
    "        print(f\"    Rows: {len(data):,}\")\n",
    "        print(f\"    Range: {data.index[0]} to {data.index[-1]}\")\n",
    "        print(f\"    Saved: {output_file}\")\n",
    "    except Exception as e:\n",
    "        print(f\"    Error: {e}\")\n",
    "\n",
    "print(\"\\nSample CSV format:\")\n",
    "sample_file = f\"{OUTPUT_DIR}/{TICKERS[0]}_1m.csv\"\n",
    "if os.path.exists(sample_file):\n",
    "    sample = pd.read_csv(sample_file, nrows=3)\n",
    "    print(sample.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "765e3c00-5af6-4338-8521-362aa3b929d5",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa01bb5-e8b8-420e-9222-7c4c5db3760c",
   "metadata": {},
   "source": [
    "## Step 2: Data Cleaning and Organization \n",
    "Objective: Prepare the raw data for modeling and strategy development.\n",
    "\n",
    "Requirements:\n",
    "\n",
    "Remove missing or duplicate rows.\n",
    "Set Datetime as index and sort chronologically.\n",
    "Add derived features (e.g., returns, moving averages).\n",
    "Deliverable:\n",
    "\n",
    "A cleaned pandas DataFrame ready for analysis.\n",
    "Example:\n",
    "\n",
    "python\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('market_data.csv')\n",
    "\n",
    "data.dropna(inplace=True)\n",
    "\n",
    "data.set_index('Datetime', inplace=True)\n",
    "\n",
    "data.sort_index(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a903829-74d3-4223-a17b-503627555add",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "89a2c9d2-24ff-4b4f-9e21-05e68b334df1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[1/9] NVDA\n",
      "    Initial: 2,814 rows\n",
      "    Final: 2,765 rows\n",
      "    Saved: cleaned_data/NVDA_cleaned.csv\n",
      "\n",
      "[2/9] AAPL\n",
      "    Initial: 2,814 rows\n",
      "    Final: 2,765 rows\n",
      "    Saved: cleaned_data/AAPL_cleaned.csv\n",
      "\n",
      "[3/9] MSFT\n",
      "    Initial: 2,812 rows\n",
      "    Final: 2,763 rows\n",
      "    Saved: cleaned_data/MSFT_cleaned.csv\n",
      "\n",
      "[4/9] GOOGL\n",
      "    Initial: 2,812 rows\n",
      "    Final: 2,763 rows\n",
      "    Saved: cleaned_data/GOOGL_cleaned.csv\n",
      "\n",
      "[5/9] AMZN\n",
      "    Initial: 2,814 rows\n",
      "    Final: 2,765 rows\n",
      "    Saved: cleaned_data/AMZN_cleaned.csv\n",
      "\n",
      "[6/9] AVGO\n",
      "    Initial: 2,814 rows\n",
      "    Final: 2,765 rows\n",
      "    Saved: cleaned_data/AVGO_cleaned.csv\n",
      "\n",
      "[7/9] META\n",
      "    Initial: 2,814 rows\n",
      "    Final: 2,765 rows\n",
      "    Saved: cleaned_data/META_cleaned.csv\n",
      "\n",
      "[8/9] TSM\n",
      "    Initial: 2,814 rows\n",
      "    Final: 2,765 rows\n",
      "    Saved: cleaned_data/TSM_cleaned.csv\n",
      "\n",
      "[9/9] BRK-B\n",
      "    Initial: 2,814 rows\n",
      "    Final: 2,765 rows\n",
      "    Saved: cleaned_data/BRK-B_cleaned.csv\n",
      "\n",
      "Sample (first ticker):\n",
      "Shape: (2765, 8)\n",
      "                                 Open        High         Low       Close  \\\n",
      "Datetime                                                                    \n",
      "2025-11-07 15:19:00+00:00  182.720001  183.210007  182.630005  182.880005   \n",
      "2025-11-07 15:20:00+00:00  182.909897  183.179993  182.850006  182.990005   \n",
      "2025-11-07 15:21:00+00:00  182.970993  183.684998  182.929993  183.295502   \n",
      "\n",
      "                           Volume   Returns       MA_20       MA_50  \n",
      "Datetime                                                             \n",
      "2025-11-07 15:19:00+00:00  367427  0.000848  182.746510  183.245296  \n",
      "2025-11-07 15:20:00+00:00  315252  0.000601  182.776971  183.222296  \n",
      "2025-11-07 15:21:00+00:00  516478  0.001669  182.802746  183.190206  \n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Step 2: Clean and Organize Data\n",
    "- Remove missing or duplicate rows\n",
    "- Set Datetime as index and sort chronologically\n",
    "- Add derived features (returns, moving averages)\n",
    "'''\n",
    "\n",
    "# Configuration\n",
    "INPUT_DIR = 'market_data'\n",
    "OUTPUT_DIR = 'cleaned_data'\n",
    "TICKERS = ['NVDA', 'AAPL', 'MSFT', 'GOOGL', 'AMZN', 'AVGO', 'META', 'TSM', 'BRK-B']\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "for i, ticker in enumerate(TICKERS, 1):\n",
    "    print(f\"\\n[{i}/{len(TICKERS)}] {ticker}\")\n",
    "    \n",
    "    input_file = f\"{INPUT_DIR}/{ticker}_1m.csv\"\n",
    "    \n",
    "    if not os.path.exists(input_file):\n",
    "        print(f\"    File not found\")\n",
    "        continue\n",
    "    \n",
    "    try:\n",
    "        # Read CSV\n",
    "        data = pd.read_csv(input_file)\n",
    "        initial_rows = len(data)\n",
    "        \n",
    "        # Set Datetime as index\n",
    "        data['Datetime'] = pd.to_datetime(data['Datetime'])\n",
    "        data.set_index('Datetime', inplace=True)\n",
    "        \n",
    "        # Sort chronologically\n",
    "        data.sort_index(inplace=True)\n",
    "        \n",
    "        # Remove duplicates\n",
    "        data = data[~data.index.duplicated(keep='first')]\n",
    "        \n",
    "        # Remove missing values\n",
    "        data.dropna(inplace=True)\n",
    "        \n",
    "        # Add derived features\n",
    "        data['Returns'] = data['Close'].pct_change()\n",
    "        data['MA_20'] = data['Close'].rolling(window=20).mean()\n",
    "        data['MA_50'] = data['Close'].rolling(window=50).mean()\n",
    "        \n",
    "        # Remove rows with NaN from indicators\n",
    "        data.dropna(inplace=True)\n",
    "        \n",
    "        final_rows = len(data)\n",
    "        print(f\"    Initial: {initial_rows:,} rows\")\n",
    "        print(f\"    Final: {final_rows:,} rows\")\n",
    "        \n",
    "        # Save cleaned data\n",
    "        output_file = f\"{OUTPUT_DIR}/{ticker}_cleaned.csv\"\n",
    "        data.to_csv(output_file)\n",
    "        print(f\"    Saved: {output_file}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"    Error: {e}\")\n",
    "\n",
    "# Show sample\n",
    "print(\"\\nSample (first ticker):\")\n",
    "sample_file = f\"{OUTPUT_DIR}/{TICKERS[0]}_cleaned.csv\"\n",
    "if os.path.exists(sample_file):\n",
    "    sample = pd.read_csv(sample_file, index_col='Datetime', parse_dates=True)\n",
    "    print(f\"Shape: {sample.shape}\")\n",
    "    print(sample.head(3))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "finm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
